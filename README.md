### Generate Test Cases Automatically from User Stories & Acceptance Criteria

This project demonstrates how to use LangChain, OpenAI GPT models, and structured output parsers to automatically generate high-quality test cases for manual and automation QA workflows.

It follows a simple architecture:

User Story + Acceptance Criteria
        â†“
LangChain Prompt Template
        â†“
LLM (GPT-4.1-mini)
        â†“
JsonOutputParser
        â†“
Structured Test Cases in JSON

ðŸ”¥ Features

Convert any user story into structured test cases.
Uses LangChain Expression Language (LCEL) for clean chaining.
Enforces strict JSON output using JsonOutputParser.

Produces:

- Happy path test cases
- Negative scenarios
- Edge case scenarios
- Easy to integrate into CI/CD, Promptfoo evaluation, or Auto-Test pipelines.

ðŸ“‚ Project Structure
project2-langchain/
â”‚â”€â”€ helper.py
â”‚â”€â”€ test_generator.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
â”‚â”€â”€ .env
â”‚â”€â”€ .gitignore

# Getting started
python3 -m venv .venv
source .venv/bin/activate      # macOS / Linux
.venv\Scripts\activate         # Windows

pip install -r requirements.txt

streamlit run helper.py